"""
Module: src.core.parsing/parse_service.py

Purpose:
    Orchestrates gem5 stats file parsing workflow using persistent Perl worker pool.
    Coordinates parallel parsing across multiple stats files and aggregates results
    into a single consolidated CSV file.

Responsibilities:
    - Submit async parsing jobs to worker pool
    - Manage variable-to-file mapping for parallel processing
    - Aggregate parsed results into final CSV
    - Handle parse cancellation and cleanup
    - Maintain variable order consistency

Dependencies:
    - PerlWorkerPool: For persistent Perl process management (54x speedup)
    - ParseWorkPool: For parallel work distribution
    - TypeMapper: For variable type mapping and validation
    - Gem5ParseWork: For individual file parsing logic

Usage Example:
    >>> from src.core.parsing.parse_service import ParseService
    >>>
    >>> # Define variables to parse
    >>> variables = [
    ...     {"name": "system.cpu.ipc", "type": "scalar", "params": {}},
    ...     {"name": "system.cpu.numCycles", "type": "scalar", "params": {}}
    ... ]
    >>>
    >>> # Submit async parsing (returns futures)
    >>> futures = ParseService.submit_parse_async(
    ...     stats_path="/path/to/gem5/output",
    ...     stats_pattern="stats.txt",
    ...     variables=variables,
    ...     output_dir="/tmp/parsed"
    ... )
    >>>
    >>> # Wait for completion and get results
    >>> results = [f.result() for f in futures]
    >>>
    >>> # Aggregate into final CSV
    >>> csv_path = ParseService.construct_final_csv("/tmp/parsed", results)

Design Patterns:
    - Service Layer Pattern: Coordinates parsing workflow
    - Async/Future Pattern: Non-blocking parallel execution
    - Worker Pool Pattern: Reuses persistent Perl processes
    - Facade Pattern: Simplifies complex parsing pipeline

Performance Characteristics:
    - Worker Pool: 54x speedup vs subprocess (30-50ms â†’ <1ms per file)
    - Parallelism: Concurrent parsing across multiple stats files
    - Scalability: O(n/p) where n=files, p=pool_size
    - Typical: 20 files in ~0.01s (vs 0.5s with subprocess)

Error Handling:
    - Raises FileNotFoundError if stats_path doesn't exist
    - Raises RuntimeError for worker pool failures
    - Logs warnings for parse errors (continues with partial results)
    - Cancellation: Terminates pending work gracefully

Thread Safety:
    - Worker pool is thread-safe (uses locks internally)
    - Service methods are stateless (except _active_var_names class var)

Configuration:
    - RING5_WORKER_POOL_SIZE: Environment variable for pool size (default: 4)

Testing:
    - Unit tests: tests/unit/test_parse_service.py
    - Integration tests: tests/integration/test_parser_integration.py
    - Performance tests: tests/performance/test_worker_pool_performance.py

Version: 2.0.0
Last Modified: 2026-01-27
"""

import logging
import os
import re
from concurrent.futures import Future
from dataclasses import replace
from typing import Any, Dict, List, Optional

from src.core.common.utils import normalize_user_path, sanitize_glob_pattern
from src.core.models import StatConfig
from src.core.parsing.gem5.impl.pool.pool import ParseWorkPool
from src.core.parsing.gem5.impl.strategies.factory import StrategyFactory

logger = logging.getLogger(__name__)

# Worker pool configuration - PRIMARY MECHANISM!

_WORKER_POOL_SIZE = int(os.environ.get("RING5_WORKER_POOL_SIZE", "4"))
logger.info(f"Worker pool is the PRIMARY parsing mechanism ({_WORKER_POOL_SIZE} workers)")


class ParseService:
    """Service for orchestrating the parsing workflow."""

    _active_var_names: List[str] = []

    @staticmethod
    def submit_parse_async(
        stats_path: str,
        stats_pattern: str,
        variables: List[StatConfig],
        output_dir: str,
        strategy_type: str = "simple",
        scanned_vars: Optional[List[Any]] = None,
    ) -> List[Future[Any]]:
        """Submit async parsing job and return futures."""
        search_path = normalize_user_path(stats_path)
        if not search_path.exists():
            raise FileNotFoundError(f"Stats path does not exist: {stats_path}")

        safe_pattern: str = sanitize_glob_pattern(stats_pattern)

        # 1. Regex Expansion (Centralized Logic)
        # If scanned_vars are provided, we expand patterns (e.g., cpu\d+)
        # before passing to the strategies.
        processed_configs = []
        for config in variables:
            expanded_config = config
            if scanned_vars and any(char in config.name for char in "*?+[]\\"):
                try:
                    logger.info(
                        f"PARSER: Matching regex '{config.name}' "
                        f"against {len(scanned_vars)} scanned variables"
                    )
                    pattern = re.compile(config.name)
                    matched_ids = []
                    for sv in scanned_vars:
                        # Handle both dict and ScannedVariable objects
                        sv_name = sv.name if hasattr(sv, "name") else str(sv.get("name", ""))
                        if config.name == sv_name or pattern.fullmatch(sv_name):
                            # If sv is already an aggregated pattern, use its constituents
                            if hasattr(sv, "pattern_indices") and sv.pattern_indices:
                                matched_ids.extend(sv.pattern_indices)
                            elif isinstance(sv, dict) and sv.get("pattern_indices"):
                                matched_ids.extend(sv.get("pattern_indices", []))
                            else:
                                matched_ids.append(sv_name)

                    if matched_ids:
                        # Success: Inject identified leaf variables into params
                        params = config.params.copy()
                        params["parsed_ids"] = matched_ids
                        expanded_config = replace(config, params=params)
                        logger.info(
                            f"PARSER: Expanded '{config.name}' to "
                            f"{len(matched_ids)} instances: {matched_ids}"
                        )
                    else:
                        logger.warning(f"PARSER: No matches found for regex '{config.name}'")
                except re.error:
                    logger.warning(f"PARSER: Invalid regex in variable: {config.name}")

            processed_configs.append(expanded_config)

        # 2. Resolve strategy via factory
        strategy = StrategyFactory.create(strategy_type)

        # 3. Get work items from strategy
        batch_work = strategy.get_work_items(stats_path, safe_pattern, processed_configs)
        if not batch_work:
            return []

        ParseService._active_var_names = [v.name for v in processed_configs]
        ParseService._last_batch_var_names = list(ParseService._active_var_names)

        pool = ParseWorkPool.get_instance()
        return pool.submit_batch_async(batch_work)

    @staticmethod
    def get_last_batch_var_names() -> List[str]:
        """Return variable names from the most recent submit_parse_async call.

        This allows callers to capture the per-batch variable ordering
        immediately after submission, instead of relying on the shared
        class-level ``_active_var_names`` which may be overwritten by a
        concurrent batch.
        """
        return list(ParseService._last_batch_var_names)

    _last_batch_var_names: List[str] = []

    @staticmethod
    def finalize_parsing(
        output_dir: str,
        results: List[Any],
        strategy_type: str = "simple",
        var_names: Optional[List[str]] = None,
    ) -> Optional[str]:
        """
        Post-process and aggregate provided results into final CSV.

        Args:
            output_dir: Directory for the output CSV file.
            results: Resolved parse results from submit_parse_async futures.
            strategy_type: Name of the strategy used for post-processing.
            var_names: Explicit variable ordering for CSV columns.
                       When *None*, falls back to ``_active_var_names``.
        """
        if not results:
            logger.warning("PARSER: No results to persist.")
            return None

        # Resolve strategy via factory for post-processing
        strategy = StrategyFactory.create(strategy_type)

        processed_results = strategy.post_process(results)
        return ParseService.construct_final_csv(
            output_dir, processed_results, var_names=var_names
        )

    @staticmethod
    def construct_final_csv(
        output_dir: str,
        results: List[Any],
        var_names: Optional[List[str]] = None,
    ) -> Optional[str]:
        """
        Aggregate provided results and save to CSV.

        Args:
            output_dir: Directory for the output CSV file.
            results: Parsed results to aggregate.
            var_names: Explicit variable ordering for CSV columns.
                       When *None*, falls back to ``_active_var_names``.
        """
        if not results:
            return None

        # Logic adapted from Gem5StatsParser._persist_results
        header_parts: List[str] = []
        sample = results[0]
        column_map: Dict[str, Optional[List[str]]] = {}

        # Use explicit var_names if provided, then fallback to class-level state
        ordered_names = var_names or ParseService._active_var_names
        # Fallback if empty
        if not ordered_names:
            ordered_names = list(sample.keys())

        for var_name in ordered_names:
            if var_name not in sample:
                continue

            var = sample[var_name]
            entries = var.entries
            if entries:
                column_map[var_name] = entries
                header_parts.extend(f"{var_name}..{e}" for e in entries)
            else:
                column_map[var_name] = None
                header_parts.append(var_name)

        os.makedirs(str(normalize_user_path(output_dir)), exist_ok=True)
        output_path = os.path.join(str(normalize_user_path(output_dir)), "results.csv")

        with open(output_path, "w", encoding="utf-8") as f:
            f.write(",".join(header_parts) + "\n")

            for file_stats in results:
                row_parts: List[str] = []
                for var_name in ordered_names:
                    if var_name not in file_stats:
                        row_parts.append("NaN")
                        continue

                    var = file_stats[var_name]

                    # Handle Stat objects vs Raw Data (from ConfigAwareStrategy)
                    if hasattr(var, "balance_content"):
                        var.balance_content()
                        var.reduce_duplicates()

                        entries = column_map[var_name]
                        if entries is not None:
                            reduced = var.reduced_content
                            for e in entries:
                                val = reduced.get(e, "NaN")
                                row_parts.append(str(val))
                        else:
                            row_parts.append(str(var.reduced_content))
                    else:
                        # Raw data (string/int/etc.)
                        row_parts.append(str(var))

                f.write(",".join(row_parts) + "\n")

        return output_path
